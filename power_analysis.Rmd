---
title: "Appendix V: Power analysis simulations for correlated variables"
output: pdf_document
---

```{r setup, include=FALSE, fig.width = 12}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r packages_functions}
library(broom)
library(plyr)
library(tidyverse)

theme_set(theme_classic())

# function to generate pairs of uncorrelated variables
sim_z <- function (n) matrix(c(rnorm(n, 0, 1), rnorm(n, 0, 1)), nrow = n)

# function to convert an uncorrelated pair of variables z to a correlated pair with
# coefficient r
corr_pair <- function(z, r) {
  # create the covariance matrix
  cormat <- matrix(1, nrow = 2, ncol = 2)
  cormat[upper.tri(cormat)] <- cormat[lower.tri(cormat)] <- r
  covmat <- cormat * tcrossprod(rep(1, 2))
  
  # calculate the Cholesky decomposition of the covariance matrix 
  ch <- chol(covmat)
  
  # we can then multiply ch by a set %>% of uncorrelated variables z to get the
  # appropriately correlated variables.
  y <- z %*% ch
}
```

## Background

The purpose of these simulations is to determine the conditions under which we are likely to find a difference between two correlated variables. The reasoning for this is that is we want to include the window-based upscaling measure and a mean value for the same variable (e.g. MW elevation and mean elevation) in the same analysis, they are often correlated. Additionally, in our paper the MW and LS versions are highly correlated (in our example, the two measures have a correlation coefficient of 0.97) and we need to know that we have the ability to detect the right effect. We are still able to detect the difference between the two approaches because of a large sample size characteristic of macroecological approaches. However, in order to understand wider applicability - and under which circumstances the method is applicable - we will do a simulation to determine, for a given correlation coefficient, at what sample size do you no longer have the ability to detect an effect. 

In each of our examples, we will simulate three variables: 2 correlated (x1, x2) and one independent (x3). We Will need to consider a range of R, a range of n, and three classes of effects (1) y ~ x1 + x3, (2) y ~ x1 + x2 + x3, (3) y ~ x1 - x2 + x3. 

## Predictors

In order to do this, we simulated several sets of predictors of varying sample size and correlation. For each pair of correlated predictors (x1, x2), we also generated a third independent predictor (x3). We therefore have a set of predictors ***X*** where $x_1$ and $x_2$ are correlated, and $x_3$ is independent. The changing parameters in the dataset simulations were:

- sample size: 30, 50, 100, 500, 1000 (for now, we may wish to extend)
- correlation: 0.7, 0.75, 0.8, 0.85, 0.9, 0.95

For each sample size, we generated 100 replicates.

```{r}
n <- c(30, 50, 100, 250, 500, 750, 1000, 2000, 5000)
r <- c(0.7, 0.75, 0.8, 0.85, 0.9, 0.95)
n_reps <- 100
rep <- 1:n_reps

param_table <- expand.grid(n = n, r = r, rep = rep)

res <- apply(param_table, 1, function(p) {
  n = p['n']
  r = p['r']
  rep = p['rep']
  z <- mapply(rnorm, n, rep(0, 3), rep(1, 3))
  x <- corr_pair(z[,1:2], r)
  out <- data.frame(n, r, rep, x, z[,3], row.names = NULL)
  names(out) <- c("n", "r", "rep","x1", "x2", "x3")
  return(out)
})

pred_df <- ldply(res)
```

## Responses

In order to understand how the ability to detect an effect might differ between models, we will generate response variables for each dataset for the following models:

- $y = 0.5x_1 + 0.5x_3 + \epsilon$ - two uncorrelated variables (similar to jay case study)
- $y = 0.5x_1 + 0.5x_2 + 0.5x_3 + \epsilon$ - two correlated variables, same effect, plus third independent variable
- $y = 0.5x_1 - 0.5x_2 + 0.5x_3 + \epsilon$ - two correlated variables, opposite effect, plus third independent variable (similar to forests case study) 

In each case here $\epsilon$ has a normal error structure with a reasonable amount of noise in the data $\epsilon\sim N(0, 1)$. We do not consider interaction terms in these simulations.

```{r}
df <- pred_df %>% 
  mutate(y1 = 0.5*x1 + 0.5*x3 + rnorm(n, 0, 1),
         y2 = 0.5*x1 + 0.5*x2 + 0.5*x3 + rnorm(n, 0, 1),
         y3 = 0.5*x1 - 0.5*x2 + 0.5*x3 + rnorm(n, 0, 1))
```

## Models

For each response, we fit a linear model of the form:

$y_i = \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \epsilon$ 

We should get the following results:

- For $y_1$, $\beta_1 = 0.5$, $\beta_2 = 0$, $\beta_3 = 0.5$
- For $y_2$, $\beta_1 = 0.5$, $\beta_2 = 0.5$, $\beta_3 = 0.5$
- For $y_3$, $\beta_1 = 0.5$, $\beta_2 = -0.5$, $\beta_3 = 0.5$

We calculate the mean of the estimates and the standard errors across the 100 replicates for final analysis. 

```{r}
df_mod <- df %>% group_by(n, r, rep) %>% 
  nest() %>% 
  mutate(fit1 = map(data, ~ lm(y1 ~ x1 + x2 + x3, data = .x)),
         fit2 = map(data, ~ lm(y2 ~ x1 + x2 + x3, data = .x)),
         fit3 = map(data, ~ lm(y3 ~ x1 + x2 + x3, data = .x)),
         mod1 = map(fit1, tidy),
         mod2 = map(fit2, tidy), 
         mod3 = map(fit3, tidy)
  ) %>% 
  select(n, r, rep, mod1, mod2, mod3) %>% 
  gather(key = model, value = result, -n, -r, -rep) %>% 
  unnest()
```

## Results

We display two plots for each model: 1) the proportion of replicates where the significance of a coefficient was correctly identified (note, for coefficients where the true value is not equal to zero, this is the number of replicates which identify the coefficient as significant, where the true value is zero, this is the number of replicates where the coefficient was correctly estimated to be non-significant); and 2) the distribution of the coefficient estimate for those replicates (out of 100) for which the significance of the estimate is correctly identified (as determined in plot 1). In all cases, the dashed line in plot 2 is at the true value for this coefficient (used to simulate the y value). 

### No effect of second correlated variable

$y = 0.5x_1 + 0.5x_3 + \epsilon$

```{r, fig.width = 12}
df <- filter(df_mod, model == "mod1", term != "(Intercept)") %>% 
  inner_join(tibble(truth = c(0.5, 0, 0.5), term = c("x1", "x2", "x3"))) %>% 
  mutate(signif = case_when(truth == 0 & p.value < 0.05 ~ "Incorrect", 
                            truth == 0 & p.value >= 0.05 ~ "Correct", 
                            truth != 0 & p.value < 0.05 ~ "Correct", 
                            truth != 0 & p.value >= 0.05 ~ "Incorrect"))

mod_vals <- filter(df, signif == "Correct") %>% 
  group_by(n, r, model, term, truth) %>% 
  summarise(mean_estimate = mean(estimate), 
            se_estimate = sd(estimate)/sqrt(n()))
  
mod_identified <- group_by(df, n, r, term, signif) %>% 
  summarise(prop = n()/n_reps)

ggplot(mod_identified, aes(x = r, y = prop, fill = signif)) + 
  geom_bar(stat = "identity") + 
  scale_fill_viridis_d(name = "Significance Identified") + 
  facet_grid(term ~ n) + 
  labs(x = "Correlation coefficient", y = "Proportion") 

ggplot(mod_vals, aes(x = r, y = mean_estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = mean_estimate - 1.96*se_estimate, 
                    ymax = mean_estimate + 1.96*se_estimate)) +
  facet_grid(term ~ n, scales = "free") +
  geom_hline(aes(yintercept = truth), colour = "red") + 
  labs(x = "Correlation coefficient", 
       y = expression(paste("Mean coefficient estimate " %+-% " 95% CI"))) 
```

### Positive effect of second correlated variable

$y = 0.5x_1 + 0.5x_2 + 0.5x_3 + \epsilon$

```{r, fig.width = 12}
df <- filter(df_mod, model == "mod2", term != "(Intercept)") %>% 
  inner_join(tibble(truth = c(0.5, 0.5, 0.5), term = c("x1", "x2", "x3"))) %>% 
  mutate(signif = case_when(truth == 0 & p.value < 0.05 ~ "Incorrect", 
                            truth == 0 & p.value >= 0.05 ~ "Correct", 
                            truth != 0 & p.value < 0.05 ~ "Correct", 
                            truth != 0 & p.value >= 0.05 ~ "Incorrect"))

mod_vals <- filter(df, signif == "Correct") %>% 
  group_by(n, r, model, term, truth) %>% 
  summarise(mean_estimate = mean(estimate), 
            se_estimate = sd(estimate)/sqrt(n()))
  
mod_identified <- group_by(df, n, r, term, signif) %>% 
  summarise(prop = n()/n_reps)

ggplot(mod_identified, aes(x = r, y = prop, fill = signif)) + 
  geom_bar(stat = "identity") + 
  scale_fill_viridis_d(name = "Significance Identified") + 
  facet_grid(term ~ n) + 
  labs(x = "Correlation coefficient", y = "Proportion") 

ggplot(mod_vals, aes(x = r, y = mean_estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = mean_estimate - 1.96*se_estimate, 
                    ymax = mean_estimate + 1.96*se_estimate)) +
  facet_grid(term ~ n, scales = "free") +
  geom_hline(aes(yintercept = truth), colour = "red") + 
  labs(x = "Correlation coefficient", 
       y = expression(paste("Mean coefficient estimate " %+-% " 95% CI"))) 
```

### Negative effect of second correlated variable

$y = 0.5x_1 - 0.5x_2 + 0.5x_3 + \epsilon$

```{r, fig.width = 12}
df <- filter(df_mod, model == "mod3", term != "(Intercept)") %>% 
  inner_join(tibble(truth = c(0.5, -0.5, 0.5), term = c("x1", "x2", "x3"))) %>% 
  mutate(signif = case_when(truth == 0 & p.value < 0.05 ~ "Incorrect", 
                            truth == 0 & p.value >= 0.05 ~ "Correct", 
                            truth != 0 & p.value < 0.05 ~ "Correct", 
                            truth != 0 & p.value >= 0.05 ~ "Incorrect"))

mod_vals <- filter(df, signif == "Correct") %>% 
  group_by(n, r, model, term, truth) %>% 
  summarise(mean_estimate = mean(estimate), 
            se_estimate = sd(estimate)/sqrt(n()))
  
mod_identified <- group_by(df, n, r, term, signif) %>% 
  summarise(prop = n()/n_reps)

ggplot(mod_identified, aes(x = r, y = prop, fill = signif)) + 
  geom_bar(stat = "identity") + 
  scale_fill_viridis_d(name = "Significance Identified") + 
  facet_grid(term ~ n) + 
  labs(x = "Correlation coefficient", y = "Proportion") 

ggplot(mod_vals, aes(x = r, y = mean_estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = mean_estimate - 1.96*se_estimate, 
                    ymax = mean_estimate + 1.96*se_estimate)) +
  facet_grid(term ~ n, scales = "free") +
  geom_hline(aes(yintercept = truth), colour = "red") + 
  labs(x = "Correlation coefficient", 
       y = expression(paste("Mean coefficient estimate " %+-% " 95% CI"))) 
```

## Conclusion

In all cases, a sample size $\geq 500$ is required to correctly identify the significance and magnitude of an effect at all correlation levels. For correlation $\leq r = 0.9$, this drops to $n = 250$. The majority of macroecological analyses will have sample sizes of this magnitude. 

## Session Info

```{r}
devtools::session_info()
```
